---
title: "AvaliaÃ§Ã£o 1 â€“ IntroduÃ§Ã£o Ã  InferÃªncia Bayesiana"
subtitle: "5290 - PROBABILIDADE E ESTATÃSTICA"
description: "Curso de Bacharelado Interdisciplinar em CiÃªncia e Tecnologia do Mar - BICT Mar"
author: "Fabio Cop Ferreira"

format:
  html:
    number-sections: true
    number-depth: 3
    include-in-header: 
      text: |
        <style>
        h2 {
          font-size: 1.1rem !important; /* h2 menor */
        }
        h3 {
          font-size: 1rem !important; /* h3 menor */
        }
        </style>

---

---

<div style="text-align: center; font-weight: bold; color: red">
RESPOSTAS DAS QUESTÃ•ES
</div>

---

## Parte A â€“ QuestÃµes conceituais (2,0 pontos)

### (0,5 ponto) Em uma anÃ¡lise bayesiana, a *verossimilhanÃ§a* de uma hipÃ³tese:

**Resposta:** <span style="color: red">B) Mede a proporÃ§Ã£o relativa de modos pelos quais a hipÃ³tese pode produzir os dados observados.</span>

### (0,5 ponto) Ao assumir uma distribuiÃ§Ã£o *a priori* uniforme sobre cinco hipÃ³teses discretas, o peso inicial atribuÃ­do a cada hipÃ³tese Ã©:

**Resposta:** <span style="color: red">B) 0,20</span>

### (0,5 ponto) Indique a alternativa que **nÃ£o** altera a distribuiÃ§Ã£o *a posteriori*:

**Resposta:** <span style="color: red">D) Multiplicar todas as probabilidades posteriores por uma constante comum.</span>

### (0,5 ponto) Quando a distribuiÃ§Ã£o *a priori* Ã© **informativa** e favorece valores altos de um parÃ¢metro, o impacto esperado sobre a distribuiÃ§Ã£o *a posteriori* Ã©:

**Resposta:** <span style="color: red">B) DeslocÃ¡â€‘la na direÃ§Ã£o dos valores altos, a depender dos dados.</span>

---

```{python}
from scipy.stats import binom
import numpy as np

y = 2
n = 3
p = np.arange(0,5)/4
priori = 1/len(p)

L = binom.pmf(y, n, p)
Post = L * priori / np.sum(L * priori)
L_round = np.round(L,3).tolist()
Post_round = np.round(Post,3).tolist()

H_selecionado = int(np.argmax(Post) + 1)
```

## Parte B â€“ Problemas quantitativos (8,0 pontos)

| HipÃ³tese | ComposiÃ§Ã£o | $n$ | $\mathcal{L}$ | Probabilidade *a posteriori* |
|:--------:|:----------:|:---:|:-------------:|:----------------------------:|
| $H_1$    | âšªâšªâšªâšª      | <span style="color: red">$0 \times 4 \times 0 = 0$</span>   | <span style="color: red">`{python} L_round[0]`</span>   | <span style="color: red">`{python} Post_round[0]`</span>  |
| $H_2$    | âšªâšªâšªğŸ”µ      | <span style="color: red">$1 \times 3 \times 1 = 3$</span>   | <span style="color: red">`{python} L_round[1]`</span>   | <span style="color: red">`{python} Post_round[1]`</span>  |
| $H_3$    | âšªâšªğŸ”µğŸ”µ      | <span style="color: red">$2 \times 2 \times 2 = 8$</span>   | <span style="color: red">`{python} L_round[2]`</span>   | <span style="color: red">`{python} Post_round[2]`</span>  |
| $H_4$    | âšªğŸ”µğŸ”µğŸ”µ      | <span style="color: red">$3 \times 1 \times 3 = 9$</span>   | <span style="color: red">`{python} L_round[3]`</span>   | <span style="color: red">`{python} Post_round[3]`</span>  |
| $H_5$    | ğŸ”µğŸ”µğŸ”µğŸ”µ      | <span style="color: red">$4 \times 0 \times 4 = 0$</span>   | <span style="color: red">`{python} L_round[4]`</span>   | <span style="color: red">`{python} Post_round[4]`</span>  |

### HipÃ³tese mais plausÃ­vel (priori uniforme):

**Resposta:** <span style="color: red">E) $H_{`{python} H_selecionado`}$</span>

---

### Probabilidade *a posteriori* com *a priori* informativa:

```{python}
from scipy.stats import binom
import numpy as np

y = 2
n = 3
p = np.arange(0,5)/4
priori_info = np.array([0.05, 0.1, 0.6, 0.15, 0.1])

L_info = binom.pmf(y, n, p)
Post_info = L_info * priori_info / np.sum(L_info * priori_info)
L_info_round = np.round(L_info,3).tolist()
Post_info_round = np.round(Post_info,3).tolist()

H_info_selecionado = int(np.argmax(Post_info) + 1)
```

| HipÃ³tese | ComposiÃ§Ã£o | Probabilidade *a priori* | Probabilidade *a posteriori* |
|:--------:|:----------:|:------------------------:|:----------------------------:|
| $H_1$    | âšªâšªâšªâšª      | 0.05                     | <span style="color: red">`{python} Post_info_round[0]`</span>  |
| $H_2$    | âšªâšªâšªğŸ”µ      | 0.10                     | <span style="color: red">`{python} Post_info_round[1]`</span>  |
| $H_3$    | âšªâšªğŸ”µğŸ”µ      | 0.60                     | <span style="color: red">`{python} Post_info_round[2]`</span>  |
| $H_4$    | âšªğŸ”µğŸ”µğŸ”µ      | 0.15                     | <span style="color: red">`{python} Post_info_round[3]`</span>  |
| $H_5$    | ğŸ”µğŸ”µğŸ”µğŸ”µ      | 0.10                     | <span style="color: red">`{python} Post_info_round[4]`</span>  |

### HipÃ³tese mais plausÃ­vel (priori informativa):

**Resposta:** <span style="color: red">E) $H_{`{python} H_info_selecionado`}$</span>
---

## Parte C â€“ Problemas de interpretaÃ§Ã£o e uso (2,0 pontos)

```{python}
import numpy as np
from scipy.stats import beta

# DefiniÃ§Ãµes dos parÃ¢metros dos cenÃ¡rios
alpha_prior = np.array([1, 9, 1, 9])
beta_prior =  np.array([1, 4, 1, 4])
y =           np.array([3, 3, 30, 30])
n =           np.array([4, 4, 40, 40])

# ParÃ¢metros da posteriori
alpha_post = alpha_prior + y
beta_post = beta_prior + (n - y)

# Intervalo desejado
lower, upper = 0.65, 0.75

# Probabilidade acumulada entre os limites
cdf_upper = beta.cdf(upper, alpha_post, beta_post)
cdf_lower = beta.cdf(lower, alpha_post, beta_post)
posterior_probs = cdf_upper - cdf_lower

posterior_probs_round = np.round(posterior_probs,3).tolist()

```

### (0,5 ponto) $n = 4$, $y = 3$, *a priori* $Beta(1, 1)$  
**Resposta:** <span style="color: red">`{python} posterior_probs_round[0]`</span>

### (0,5 ponto) $n = 4$, $y = 3$, *a priori* $Beta(9, 4)$  
**Resposta:** <span style="color: red">`{python} posterior_probs_round[1]`</span>

### (0,5 ponto) $n = 40$, $y = 30$, *a priori* $Beta(1, 1)$  
**Resposta:** <span style="color: red">`{python} posterior_probs_round[2]`</span>

### (0,5 ponto) $n = 40$, $y = 30$, *a priori* $Beta(9, 4)$  
**Resposta:** <span style="color: red">`{python} posterior_probs_round[3]`</span>

---
